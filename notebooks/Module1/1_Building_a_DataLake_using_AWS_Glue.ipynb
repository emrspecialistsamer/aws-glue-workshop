{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Data Lake using AWS Glue <a name=\"top\"></a>\n",
    "\n",
    "## Table of Contents:\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Activity 1 : CSV to Parquet conversion](#Activity-1-:-CSV-to-Parquet-conversion)\n",
    "3. [Activity 2 : Building a Star Schema in your Datalake](#Activity-2-:-Building-a-Star-Schema-in-your-Datalake)\n",
    "3. [Activity 3 : Building an AWS Glue Workflow](#Activity-3-:-Building-an-AWS-Glue-Workflow)\n",
    "4. [Wrap-up](#Wrap-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "[(Back to the top)](#top)\n",
    "\n",
    "In this notebook, we will use AWS Glue to perform 3 activities:\n",
    "    \n",
    "- Convert a CSV Dataset to Parquet partitioned out by key fields.\n",
    "- Build a Star (Denormalized) Schema from an OLTP 3NF (3rd Normal Form) Schema.\n",
    "- Finally, deploy the piplline to create an AWS Glue Workflow.\n",
    "\n",
    "Let's start by connecting to our our AWS Glue Dev Endpoint - a persistent AWS Glue Spark  Development environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:13:36.786585Z",
     "start_time": "2020-05-20T20:13:12.192109Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:14:24.960531Z",
     "start_time": "2020-05-20T20:14:21.561942Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:14:26.609627Z",
     "start_time": "2020-05-20T20:14:25.773077Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that regular Spark SQL commands work great as we have enabled the feature 'Use Glue Data Catalog as the Hive metastore' for our AWS Glue Dev Endpoint. \n",
    "\n",
    "You can click on the link to read more on [AWS Glue Data Catalog Support for Spark SQL Jobs](\n",
    "https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-glue-data-catalog-hive.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 1 : CSV to Parquet conversion\n",
    "[(Back to the top)](#top)\n",
    "\n",
    "The 1st dataset we will be using is the NYC Taxi Trips CSV dataset with 1.2B records. We will partition the data in the analytics tier by vendor name, year and month, catalog this data in the AWS Glue Data Catalog. This dataset has 5 vendors and 8 years of data.\n",
    "\n",
    "We will perform the following 3 steps to make the final Parquet converted data available as an AWS Glue Table.\n",
    "\n",
    "<img src=\"../resources/activity_flow_1.png\" alt=\"Module1 Flow\" style=\"width: 350px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl the Source Data\n",
    "\n",
    "The 1st step is to run the AWS Glue Crawler on the raw dataset to create the table in the AWS Glue Catalog.\n",
    "\n",
    "Create and Execute a AWS Glue Crawler on the source data in S3\n",
    "\n",
    "- Navigate to the AWS Glue console at Services -> AWS Glue\n",
    "- From the left-hand panel menu, navigate to Data Catalog -> Crawlers.\n",
    "- Click on the button ‘Add Crawler’ to create a new AWS Glue Crawler.\n",
    "- Fields to fill in:\n",
    "    - Page: Add information about your crawler\n",
    "        - Crawler name: **nyc_trips_csv_crawler**\n",
    "    - Page: Add a data store\n",
    "        - Choose a data store: S3\n",
    "        - Include path: **s3://###s3_bucket###/data/nyc_trips_csv/**\n",
    "    - Page: Choose an IAM role\n",
    "       - IAM Role: Choose an existing IAM role **###iam_role###**\n",
    "    - Page: Configure the crawler's output\n",
    "        - Database: Click on ‘Add database’ and enter database name as **nyc_trips**.\n",
    "- Click on the button ‘Finish’ to create the crawler.\n",
    "- Select the new Crawler and click on 'Run crawler' to run the Crawler.\n",
    "\n",
    "Once the data is crawled, which should take about a minute, we can view the database and tables in the AWS Glue Catalog and query the tables as well:\n",
    "\n",
    "### Transform the data to Parquet\n",
    "\n",
    "Let's query the table created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:19:29.968851Z",
     "start_time": "2020-05-20T20:19:29.628736Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"use nyc_trips\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:19:32.361576Z",
     "start_time": "2020-05-20T20:19:32.038082Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:19:35.717069Z",
     "start_time": "2020-05-20T20:19:34.885766Z"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"select * from nyc_trips.nyc_trips_csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now write an AWS Glue Spark job to convert this csv data into a columnar(parquet) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:19:38.734325Z",
     "start_time": "2020-05-20T20:19:38.618397Z"
    }
   },
   "outputs": [],
   "source": [
    "## We will simulate the Glue job arguments \n",
    "import sys\n",
    "sys.argv = [\"CSV2Parquet\",\"--JOB_NAME\", \"CSV2Parquet\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the code for the AWS Glue Job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:19:42.008393Z",
     "start_time": "2020-05-20T20:19:41.180254Z"
    }
   },
   "outputs": [],
   "source": [
    "## Glue boilerplate code\n",
    "\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "import boto3, json\n",
    "\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "print (args['JOB_NAME']+\" START...\")\n",
    "if 'sc' not in vars(): sc = SparkContext()\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "## Glue boilerplate code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:19:44.042440Z",
     "start_time": "2020-05-20T20:19:43.926373Z"
    }
   },
   "outputs": [],
   "source": [
    "db_name='nyc_trips'\n",
    "tbl_name='nyc_trips_csv'\n",
    "output_dir='s3://###s3_bucket###/data/nyc_trips_parquet/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily instantiate an AWS Glue DynamicFrame from the AWS Glue Catalog table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:20:35.917094Z",
     "start_time": "2020-05-20T20:19:46.342618Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read the input data\n",
    "dyf = glueContext.create_dynamic_frame.from_catalog(database = db_name, table_name = tbl_name)\n",
    "dyf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are not doing any transformations here, we can write out the data out to our Amazon S3 bucket in Parquet right way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:21:23.421831Z",
     "start_time": "2020-05-20T20:20:35.918840Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write the data out in Parquet\n",
    "glueContext.write_dynamic_frame.from_options(frame = dyf, connection_type = \"s3\", connection_options = {\"path\": output_dir, \"partitionKeys\": ['vendor_name', 'year', 'month']}, format = \"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:21:23.578831Z",
     "start_time": "2020-05-20T20:21:23.423566Z"
    }
   },
   "outputs": [],
   "source": [
    "## Glue boilerplate code\n",
    "\n",
    "job.commit()\n",
    "print (args['JOB_NAME']+\" END...\")\n",
    "\n",
    "## Glue boilerplate code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl the Transformed Data\n",
    "\n",
    "Now that the output data is in Amazon S3, let's crawl this dataset in AWS Glue and query this data using Amazon Athena.\n",
    "\n",
    "- Navigate to the AWS Glue console at Services -> AWS Glue\n",
    "- From the left-hand panel menu, navigate to Data Catalog -> Crawlers.\n",
    "- Click on the button ‘Add Crawler’ to create a new AWS Glue Crawler.\n",
    "- Fields to fill in:\n",
    "    - Page: Add information about your crawler\n",
    "        - Crawler name: **nyc_trips_parquet_crawler**\n",
    "    - Page: Add a data store\n",
    "        - Choose a data store: S3\n",
    "        - Include path: **s3://###s3_bucket###/data/nyc_trips_parquet/**\n",
    "    - Page: Choose an IAM role\n",
    "       - IAM Role: Choose an existing IAM role **glue-labs-GlueServiceRole**\n",
    "    - Page: Configure the crawler's output\n",
    "        - Database: Select database as **nyc_trips**\n",
    "- Click on the button ‘Finish’ to create the crawler.\n",
    "- Select the new Crawler and click on Run crawler to run the Crawler.\n",
    "\n",
    "Once the Crawler run has completed, navigate to the Amazon Athena console, Services -> Athena to run Amazon Athena queries on this dataset.\n",
    "\n",
    "Note: You may need to output location for Amazon Athena by clicking on Settings -> Query result location in the Amazon Athena console and setting the value to : \n",
    "\n",
    "**s3://###s3_bucket###/athena-query-results/**\n",
    "\n",
    "We can also query the data using Spark SQL:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:24:14.038311Z",
     "start_time": "2020-05-20T20:24:13.162689Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:24:36.331929Z",
     "start_time": "2020-05-20T20:24:16.920360Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select count(*) from nyc_trips_parquet\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 2 : Building a Star Schema in your Datalake\n",
    "[(Back to the top)](#top)\n",
    "\n",
    "In this activity, we will denormalize an OLTP 3NF schema to Parquet. This activity demonstrates the using AWS Glue operations to perform powerful data transformations on input data:\n",
    "\n",
    "![alt text](../resources/denormalize.png \"Building a Star Schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Crawl the Source Data\n",
    "\n",
    "The 1st step is to run the AWS Crawler on the raw dataset to create the tables in the AWS Glue Catalog.\n",
    "\n",
    "- Navigate to the AWS Glue console at Services -> AWS Glue\n",
    "- From the left-hand panel menu, navigate to Data Catalog -> Crawlers.\n",
    "- Click on the button ‘Add Crawler’ to create a new AWS Glue Crawler.\n",
    "- Fields to fill in:\n",
    "    - Page: Add information about your crawler\n",
    "        - Crawler name: **salesdb_crawler**\n",
    "    - Page: Add a data store\n",
    "        - Choose a data store: S3\n",
    "        - Include path: **s3://###s3_bucket###/data/salesdb/**\n",
    "    - Page: Choose an IAM role\n",
    "       - IAM Role: Choose an existing IAM role **###iam_role###**\n",
    "    - Page: Configure the crawler's output\n",
    "        - Database:  Click on ‘Add database’ and enter database name as **salesdb**.\n",
    "- Click on the button ‘Finish’ to create the crawler.\n",
    "- Select the new Crawler and click on Run crawler to run the Crawler.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:28:10.219411Z",
     "start_time": "2020-05-20T20:28:09.349347Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"use salesdb\").show()\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Transform the dataset\n",
    "\n",
    "Let's now denormalize the source tables where applicable and write out the data in Parquet format to the destination location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:31:53.789869Z",
     "start_time": "2020-05-20T20:28:13.599887Z"
    }
   },
   "outputs": [],
   "source": [
    "db_name='salesdb'\n",
    "table1='customer'\n",
    "table2='customer_site'\n",
    "output_dir='s3://###s3_bucket###/data/sales_analytics/customer_dim'\n",
    "print (output_dir)\n",
    "\n",
    "# Read the Source Tables\n",
    "cust_dyf = glueContext.create_dynamic_frame.from_catalog(database = db_name, table_name = table1)\n",
    "cust_site_dyf = glueContext.create_dynamic_frame.from_catalog(database = db_name, table_name = table2)\n",
    "\n",
    "# Join the two Source Tables\n",
    "customer_dim_dyf = Join.apply(cust_dyf,cust_site_dyf,\n",
    "                       'cust_id', 'cust_id').drop_fields(['cust_id'])\n",
    "\n",
    "# Write the denormalized CUSTOMER_DIM table in Parquet\n",
    "glueContext.write_dynamic_frame.from_options(frame = customer_dim_dyf, connection_type = \"s3\", connection_options = {\"path\": output_dir}, format = \"parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:32:13.410229Z",
     "start_time": "2020-05-20T20:32:08.057009Z"
    }
   },
   "outputs": [],
   "source": [
    "table1='product_category'\n",
    "table2='product'\n",
    "output_dir='s3://###s3_bucket###/data/sales_analytics/product_dim/'\n",
    "print (output_dir)\n",
    "\n",
    "# Read the Source Tables\n",
    "table1_dyf = glueContext.create_dynamic_frame.from_catalog(database = db_name, table_name = table1)\n",
    "table2_dyf = glueContext.create_dynamic_frame.from_catalog(database = db_name, table_name = table2)\n",
    "\n",
    "#Join the Source Tables\n",
    "product_dim_dyf = Join.apply(table1_dyf,table2_dyf,\n",
    "                       'category_id', 'category_id').drop_fields(['category_id'])\n",
    "\n",
    "# Write the denormalized CUSTOMER_DIM table in Parquet\n",
    "glueContext.write_dynamic_frame.from_options(frame = product_dim_dyf, connection_type = \"s3\", connection_options = {\"path\": output_dir}, format = \"parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:32:44.853461Z",
     "start_time": "2020-05-20T20:32:41.508965Z"
    }
   },
   "outputs": [],
   "source": [
    "table1='supplier'\n",
    "output_dir='s3://###s3_bucket###/data/sales_analytics/supplier_dim/'\n",
    "print (output_dir)\n",
    "\n",
    "# Read the Source Tables\n",
    "table1_dyf = glueContext.create_dynamic_frame.from_catalog(database = db_name, table_name = table1)\n",
    "\n",
    "\n",
    "# Write the denormalized CUSTOMER_DIM table in Parquet\n",
    "glueContext.write_dynamic_frame.from_options(frame = table1_dyf, connection_type = \"s3\", connection_options = {\"path\": output_dir}, format = \"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:32:45.903127Z",
     "start_time": "2020-05-20T20:32:45.787803Z"
    }
   },
   "outputs": [],
   "source": [
    "table1='sales_order_detail'\n",
    "table2='sales_order'\n",
    "output_dir='s3://###s3_bucket###/data/sales_analytics/sales_order_fact/'\n",
    "print (output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 'sales_order_fact' table, we will try a different approach - \n",
    "\n",
    "- We will convert the AWS Glue DynamicFrame to a Spark DataFrame\n",
    "- Register the Spark Dataframe to a Spark Temporary View\n",
    "- Use Spark SQL to build the write out the target dataset.\n",
    "\n",
    "This demonstrates that AWS Glue DynamicFrames and Spark Dataframes are interchangeable and you can get the best of both worlds by using both the options where suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:32:50.925429Z",
     "start_time": "2020-05-20T20:32:48.590150Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read the Source Tables\n",
    "table1_dyf = glueContext.create_dynamic_frame.from_catalog(database = db_name, table_name = table1)\n",
    "table2_dyf = glueContext.create_dynamic_frame.from_catalog(database = db_name, table_name = table2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:32:51.626965Z",
     "start_time": "2020-05-20T20:32:51.514043Z"
    }
   },
   "outputs": [],
   "source": [
    "table1_dyf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:32:54.690879Z",
     "start_time": "2020-05-20T20:32:54.575708Z"
    }
   },
   "outputs": [],
   "source": [
    "table2_dyf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:32:57.570646Z",
     "start_time": "2020-05-20T20:32:57.458504Z"
    }
   },
   "outputs": [],
   "source": [
    "table1_dyf.toDF().createOrReplaceTempView(\"sales_order_v\")\n",
    "table2_dyf.toDF().createOrReplaceTempView(\"sales_order_detail_v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:33:09.244279Z",
     "start_time": "2020-05-20T20:32:59.877787Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write the denormalized SALES_ORDER_FACT table\n",
    "df=spark.sql(\"SELECT a.*, b.site_id, b.order_date,b.ship_mode \\\n",
    "FROM sales_order_detail_v b, sales_order_v a \\\n",
    "WHERE a.order_id=b.order_id\")\n",
    "df.printSchema()\n",
    "print(df.count())\n",
    "df.coalesce(1).write.mode(\"OVERWRITE\").parquet(\"s3://###s3_bucket###/data/sales_analytics/sales_order_fact/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we used the power of Spark SQL for this transformation instead of AWS Glue DynamicFrame transforms. This dataset is small so we also coalesced the number of partitions in the Spark dataframe to 1 to ensure only 1 file gets written to our output location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:33:13.463204Z",
     "start_time": "2020-05-20T20:33:12.735834Z"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "aws s3 ls s3://###s3_bucket###/data/sales_analytics/sales_order_fact/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the output data is in Amazon S3, let's crawl this dataset in AWS Glue and query this data using Amazon Athena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Crawl the Transformed Data\n",
    "\n",
    "- Navigate to the Glue console at Services -> Glue\n",
    "- From the left-hand panel menu, navigate to Data Catalog -> Crawlers.\n",
    "- Click on the button ‘Add Crawler’ to create a new Glue Crawler.\n",
    "- Fields to fill in:\n",
    "    - Page: Add information about your crawler\n",
    "        - Crawler name: **sales_analytics_crawler**\n",
    "    - Page: Add a data store\n",
    "        - Choose a data store: S3\n",
    "        - Include path: **s3://###s3_bucket###/data/sales_analytics/**\n",
    "    - Page: Choose an IAM role\n",
    "        - IAM Role: Choose an existing IAM role **###iam_role###**\n",
    "    - Page: Configure the crawler's output\n",
    "        - Database:  Click on ‘Add database’ and enter database name as **sales_analytics**.\n",
    "- Click on the button ‘Finish’ to create the crawler.\n",
    "- Select the new Crawler and click on Run crawler to run the Crawler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:35:32.257757Z",
     "start_time": "2020-05-20T20:35:31.387873Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"use sales_analytics\").show()\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 3 : Building an AWS Glue Workflow\n",
    "[(Back to the top)](#top)\n",
    "\n",
    "An AWS Glue workflow is an orchestration used to visualize and manage the relationship and execution of multiple AWS Glue triggers, jobs and crawlers. Let's now build an AWS Glue Workflow for the same. \n",
    "\n",
    "The 1st step is to create the AWS Glue Jobs. As the AWS Glue ETL code is already staged in our Amazon S3 bucket, we will simply call the AWS Glue APIs to create the AWS Glue Jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:35:38.089828Z",
     "start_time": "2020-05-20T20:35:37.217679Z"
    }
   },
   "outputs": [],
   "source": [
    "%local\n",
    "\n",
    "import boto3\n",
    "\n",
    "acct_number=boto3.client('sts').get_caller_identity().get('Account')\n",
    "bucket='###s3_bucket###'\n",
    "iam_role='###iam_role###'\n",
    "\n",
    "# Create the AWS Glue Spark Jobs\n",
    "glue = boto3.client(\"glue\")\n",
    "\n",
    "for job_name in ['Load_SALES_ORDER_FACT', 'Load_PRODUCT_DIM', 'Load_CUSTOMER_DIM','Load_SUPPLIER_DIM']:\n",
    "    response=glue.create_job(Name=job_name,\n",
    "                         Role=f\"arn:aws:iam::{acct_number}:role/{iam_role}\",\n",
    "                         ExecutionProperty={'MaxConcurrentRuns': 1},\n",
    "                         Command={'Name': 'glueetl',\n",
    "                                  'ScriptLocation': f's3://{bucket}/scripts/{job_name}.py',\n",
    "                                  'PythonVersion': '3'},\n",
    "                         DefaultArguments={'--TempDir': f's3://{bucket}/temp',\n",
    "                                           '--enable-continuous-cloudwatch-log': 'true',\n",
    "                                           '--enable-glue-datacatalog': '',\n",
    "                                           '--enable-metrics': '',\n",
    "                                           '--enable-spark-ui': 'true',\n",
    "                                           '--spark-event-logs-path': f's3://{bucket}/spark_glue_etl_logs/{job_name}',\n",
    "                                           '--job-bookmark-option': 'job-bookmark-disable',\n",
    "                                           '--job-language': 'python',\n",
    "                                           '--S3_BUCKET': bucket },\n",
    "                         MaxRetries=0,\n",
    "                         Timeout=2880,\n",
    "                         MaxCapacity=3.0,\n",
    "                         GlueVersion='1.0',\n",
    "                         Tags={'Owner': 'Glue_Labs'}\n",
    "                        )\n",
    "    print (response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Workflow consists of 3 AWS Glue triggers:\n",
    "\n",
    "- The 1st OnDemand Trigger loads the Dimension tables.\n",
    "- The 2nd Conditional Trigger loads the Fact table.\n",
    "- The 3rd Conditional Trigger updated the table definitions in the Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:35:42.620317Z",
     "start_time": "2020-05-20T20:35:42.126763Z"
    }
   },
   "outputs": [],
   "source": [
    "%local\n",
    "\n",
    "glue = boto3.client(\"glue\")\n",
    "\n",
    "# Create the AWS Glue Workflow\n",
    "response = glue.create_workflow(\n",
    "    Name='Sales_Analytics_Workflow',\n",
    "    Description='Sales Analytics Workflow v1.0'\n",
    ")\n",
    "print (response)\n",
    "\n",
    "# 1. The Trigger to load the Dimensions table\n",
    "response = glue.create_trigger(\n",
    "    Name='1_Load_Dimensions',\n",
    "    WorkflowName='Sales_Analytics_Workflow',\n",
    "    Type='ON_DEMAND',\n",
    "    Actions=[{'JobName': 'Load_CUSTOMER_DIM',\n",
    "    'Arguments': {'--job-bookmark-option': 'job-bookmark-disable'},\n",
    "    'Timeout': 2880},\n",
    "   {'JobName': 'Load_PRODUCT_DIM',\n",
    "    'Arguments': {'--job-bookmark-option': 'job-bookmark-disable'},\n",
    "    'Timeout': 2880},\n",
    "   {'JobName': 'Load_SUPPLIER_DIM',\n",
    "    'Arguments': {'--job-bookmark-option': 'job-bookmark-disable'},\n",
    "    'Timeout': 2880}]\n",
    ")\n",
    "print (response)  \n",
    "\n",
    "# 2. The Trigger to load the Facts table\n",
    "response = glue.create_trigger(\n",
    "    Name='2_Load_Facts',\n",
    "    WorkflowName='Sales_Analytics_Workflow',\n",
    "    Type='CONDITIONAL',\n",
    "    StartOnCreation=True,\n",
    "    Actions=[{'JobName': 'Load_SALES_ORDER_FACT'}],\n",
    "    Predicate= {'Logical': 'AND',\n",
    "    'Conditions': [{'LogicalOperator': 'EQUALS',\n",
    "                  'JobName': 'Load_SUPPLIER_DIM',\n",
    "                   'State': 'SUCCEEDED'},\n",
    "                  {'LogicalOperator': 'EQUALS',\n",
    "                   'JobName': 'Load_PRODUCT_DIM',\n",
    "                   'State': 'SUCCEEDED'},\n",
    "                  {'LogicalOperator': 'EQUALS',\n",
    "                   'JobName': 'Load_CUSTOMER_DIM',\n",
    "                   'State': 'SUCCEEDED'}]\n",
    "               }\n",
    ")\n",
    "print (response)  \n",
    "\n",
    "# Finally, the Trigger for the Crawler\n",
    "response = glue.create_trigger(\n",
    "    Name='3_Update_Catalog',\n",
    "    WorkflowName='Sales_Analytics_Workflow',\n",
    "    Type='CONDITIONAL',\n",
    "    StartOnCreation=True,\n",
    "    Actions=[{'CrawlerName': 'sales_analytics_crawler'}],\n",
    "    Predicate= {'Logical': 'ANY',\n",
    "   'Conditions': [{'LogicalOperator': 'EQUALS',\n",
    "     'JobName': 'Load_SALES_ORDER_FACT',\n",
    "     'State': 'SUCCEEDED'}]}\n",
    ")\n",
    "print (response)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review the AWS Glue Workflow created:\n",
    "    \n",
    "- Navigate to the Glue Console at Service -> Glue\n",
    "- From the left-hand panel menu, choose Workflows\n",
    "- Select the Workflow 'Sales_Analytics_Workflow'.\n",
    "\n",
    "Your workflow should look like this:\n",
    "\n",
    "![title](../resources/Glue_Workflow.png)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now run this workflow: \n",
    "\n",
    "- Select the workflow and click on 'Action - > Run' to launch the workflow\n",
    "- You can view the run details and visually track the progress of each acitvity in the workflow from the 'History' tab by selecting the workflow run and clicking on 'View Run Details'\n",
    "\n",
    "![title](../resources/View_Run_Details.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "[(Back to the top)](#top)\n",
    "\n",
    "\n",
    "In this notebook, we ran exercises to perform: \n",
    "\n",
    "1. A CSV to Parquet conversion and observed how easy it is to transform and write data to an Amazon S3 bucket using AWS Glue, partitioned by key fields.\n",
    "2. A more complex transformation - denormalizing of a 3NF OLTP schema, and we observed how easy it is to perform complex data transformations using the power of both AWS Glue DynamicFrames and Spark SQL.\n",
    "3. We built and executed an AWS Glue Workflow to orchestrate multiple AWS Glue Jobs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
