{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Source and Target Databases and the Ingestion Pipeline <a name=\"top\"></a>\n",
    "\n",
    "## Table of Contents:\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "2. [Aurora MySQL as Source Database](#Aurora-MySQL-as-Source-Database)\n",
    "3. [Amazon Redshift as Data Warehouse](#Amazon-Redshift-as-Data-Warehouse)\n",
    "4. [AWS DMS as Near Real-Time Ingestion Pipeline](#AWS-DMS-as-Near-Real-Time-Ingestion-Pipeline)\n",
    "5. [Simulate Inserts](#Simulate-Inserts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "[(Back to the top)](#top)\n",
    "\n",
    "We will use this notebook to verify that our Aurora MySQL and Amazon Redshift Databases are up and running. \n",
    "\n",
    "The Aurora MySQL database will serve as the source of transactions, and the Amazon Redshift database will serve as the target Data Warehouse. We will execute inserts from this notebook as well to simulate new transactions.\n",
    "\n",
    "<img src=\"../resources/module2_architecture_diagram.png\" alt=\"Module2 Architecture Diagram]\" style=\"width: 1000px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T23:06:38.363948Z",
     "start_time": "2020-05-19T23:06:38.358989Z"
    }
   },
   "source": [
    "## Aurora MySQL as Source Database\n",
    "[(Back to the top)](#top)\n",
    "    \n",
    "Let's first test connectivity to our database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T16:13:13.727859Z",
     "start_time": "2020-05-22T16:13:13.652228Z"
    }
   },
   "outputs": [],
   "source": [
    "import MySQLdb,random,time\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "host = '###mysql_host###'\n",
    "user = 'master'\n",
    "password = 'S3cretPwd99'\n",
    "port = 3306\n",
    "db = 'salesdb'\n",
    "\n",
    "conn = MySQLdb.Connection(\n",
    "    host=host,\n",
    "    user=user,\n",
    "    passwd=password,\n",
    "    port=port,\n",
    "    db=db\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run some SQL statements. We will use the following helper functions to execute SQL statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:45:13.816809Z",
     "start_time": "2020-05-20T20:45:13.812927Z"
    }
   },
   "outputs": [],
   "source": [
    "def execute_sql(sql):\n",
    "    results=[]\n",
    "    conn.query(sql)\n",
    "    result = conn.store_result()\n",
    "    for i in range(result.num_rows()):\n",
    "        r = result.fetch_row()\n",
    "        #print(r)\n",
    "        results.append(r)\n",
    "    return results\n",
    "        \n",
    "def execute_dml(sql):\n",
    "    conn.query(sql)\n",
    "    rowcount = conn.affected_rows()\n",
    "    print (\"Rows updated: %d\"%rowcount)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:45:16.524891Z",
     "start_time": "2020-05-20T20:45:16.513741Z"
    }
   },
   "outputs": [],
   "source": [
    "execute_sql(\"show tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a generic SALES OLTP schema. Of the tables above, the SALES_ORDER_DETAIL is the one for which we will be inserting records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Redshift as Data Warehouse\n",
    "[(Back to the top)](#top)\n",
    "\n",
    "Let's test connectivity to our target datawarehouse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:47:19.961127Z",
     "start_time": "2020-05-20T20:47:19.917156Z"
    }
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "rs_host='###redshift_host###'\n",
    "rs_dbname='sales_analytics_dw'\n",
    "rs_user = 'awsuser'\n",
    "rs_password = 'S3cretPwd99'\n",
    "rs_port = 5439\n",
    "\n",
    "con=psycopg2.connect(dbname=rs_dbname, host=rs_host, port=rs_port, user=rs_user, password=rs_password)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the connectivity to the Amazon Redshift database is working fine, let's load the schema to the Amazon Redshift database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:45:23.187676Z",
     "start_time": "2020-05-20T20:45:22.956185Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_redshift_schemas(conn, scriptFileName):\n",
    "    with open (scriptFileName, \"r\") as scriptfile:\n",
    "        contents=scriptfile.read()\n",
    "    commands=[]\n",
    "    for sql in str(contents).split(\";\")[:-1]:\n",
    "        commands.append(sql)\n",
    "    for sql in commands:\n",
    "        cursor = con.cursor()\n",
    "        cursor.execute(sql)\n",
    "        cursor.close()\n",
    "        # commit the changes\n",
    "        con.commit()\n",
    "    cursor = con.cursor()\n",
    "    cursor.execute(\"Select distinct tablename from PG_TABLE_DEF where schemaname = 'public'\")\n",
    "    rows = cursor.fetchall()\n",
    "    for row in rows:\n",
    "        print (\"   \", row)\n",
    "    cursor.close()\n",
    "\n",
    "load_redshift_schemas(con,'redshift-schema.sql')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS DMS as Near Real-Time Ingestion Pipeline\n",
    "[(Back to the top)](#top)\n",
    "\n",
    "In this step we will execute a full load of data from this database to Amazon S3 using AWS DMS:\n",
    "\n",
    "- Navigate to the DMS Console by clicking on Services -> DMS. \n",
    "- Locate the menu item Conversion & migration->Database migration tasks from the left-hand panel of the DMS Console.\n",
    "- Select the only Replication Task item and click on the button Actions -> Restart/Resume to start this task.\n",
    "- You can monitor the progress of this task by clicking on the task link and viewing the 'Table Statistics' tab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Inserts\n",
    "[(Back to the top)](#top)\n",
    "\n",
    "Let's perform some Inserts to our data. We will use the helper function below to perform the inserts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T21:17:33.739244Z",
     "start_time": "2020-05-20T21:17:33.734821Z"
    }
   },
   "outputs": [],
   "source": [
    "def insert_orders(order_id,new_order_id,max_line_id):\n",
    "    print (new_order_id)\n",
    "    execute_dml(f\"insert into SALES_ORDER( ORDER_ID, SITE_ID,ORDER_DATE,SHIP_MODE ) select {new_order_id},  SITE_ID,ORDER_DATE,SHIP_MODE from SALES_ORDER where ORDER_ID={order_id}\")\n",
    "    execute_dml(f\"insert into SALES_ORDER_DETAIL( ORDER_ID, LINE_ID,LINE_NUMBER,PRODUCT_ID,QUANTITY,UNIT_PRICE,DISCOUNT,SUPPLY_COST,TAX,ORDER_DATE ) select {new_order_id}, {max_line_id}+LINE_ID,LINE_NUMBER,PRODUCT_ID,QUANTITY,UNIT_PRICE,DISCOUNT,SUPPLY_COST,TAX,ORDER_DATE from SALES_ORDER_DETAIL where ORDER_ID={order_id}\")   \n",
    "    \n",
    "def generate_orders(n):\n",
    "    new_order_id=execute_sql('select max(order_id) FROM SALES_ORDER')[0][0][0]\n",
    "    max_line_id=execute_sql('select max(line_id) FROM SALES_ORDER_DETAIL')[0][0][0]\n",
    "    print (f\"max_line_id : {max_line_id}\")\n",
    "    for i in tqdm(range(n)):\n",
    "        order_id=random.randint(1,29000)\n",
    "        new_order_id +=1\n",
    "        insert_orders(order_id,new_order_id,max_line_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T16:13:27.490811Z",
     "start_time": "2020-05-22T16:13:21.115428Z"
    }
   },
   "outputs": [],
   "source": [
    "generate_orders(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the updates that AWS DMS has pushed through using the 'Table Statistics' tab for the Replication task within the AWS DMS Console.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Note:</b> Please keep this notebook open as we move to the 2nd notebook in this Module to execute the AWS Glue incremental ETL jobs. We will execute the cell above again when we need to insert more data.</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
