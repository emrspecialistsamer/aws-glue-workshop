{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute Incremental Processing Job with AWS Glue <a name=\"top\"></a>\n",
    "\n",
    "## Table of Contents:\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "2. [Execute the Full Load Pipeline](#Execute-the-Full-Load-Pipeline)\n",
    "3. [Execute the Incremental Load Pipeline](#Execute-the-Incremental-Load-Pipeline)\n",
    "4. [Wrap-up](#Wrap-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "[(Back to the top)](#top)\n",
    "\n",
    "We will continue this module to implement the data pipeline below in this notebook. \n",
    "\n",
    "<img src=\"../resources/module2_architecture_diagram.png\" alt=\"Module2 Architecture Diagram]\" style=\"width: 1000px;\"/>\n",
    "\n",
    "In this notebook, we will run the following steps :\n",
    "\n",
    "* Define a AWS Glue Database Connection for the Amazon Redshift Database.\n",
    "* Crawl the Amazon Redshift Database to load the tables in the AWS Glue Catalog.\n",
    "* Execute the full load job.\n",
    "* Deploy and execute the incremental job with AWS Glue Bookmarking enabled.\n",
    "* Execute some Insert statements.\n",
    "* Crawl the incremental data tables. \n",
    "* Run the incremental job and Validate Results.\n",
    "* Repeat steps 5 and 6 to demonstrate the AWS Glue Bookmarking feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the Full Load Pipeline\n",
    "[(Back to the top)](#top)\n",
    "\n",
    "\n",
    "#### Define the AWS Glue Database Connection\n",
    "\n",
    "**Step 1**: Create a AWS Glue Database connection to the Amazon Redshift Database:\n",
    "\n",
    "- Navigate to the AWS Glue console at Services -> AWS Glue\n",
    "- From the left-hand panel menu, navigate to Data Catalog -> Databases -> Connections.\n",
    "- Click on the button ‘Add Connection’ to create a new AWS Glue Database Connection.\n",
    "- Fields to fill in:\n",
    "   - Page:  Set up your connection’s properties. \n",
    "      - Connection name: **redshiftdb**\n",
    "      - Connection type: **Amazon Redshift**\n",
    "   - Page: Set up access to your data store.\n",
    "      - Cluster: Select the Redshift Cluster\n",
    "      - Database name: **sales_analytics_dw**\n",
    "      - Username: **awsuser**\n",
    "      - Password: **S3cretPwd99**\n",
    "- Click on the button ‘Finish’ to create the AWS Glue Database Connection.\n",
    "- Select the new AWS Glue Database Connection and click on 'Test Connection' to test connectivity.\n",
    "\n",
    "#### Crawl the Redshift Schema\n",
    "\n",
    "\n",
    "**Step 2**: Let's run a AWS Glue Crawler on the schema in the Amazon Redshift Database:\n",
    "\n",
    "- Navigate to the AWS Glue console at Services -> AWS Glue\n",
    "- From the left-hand panel menu, navigate to Data Catalog -> Crawlers.\n",
    "- Click on the button ‘Add Crawler’ to create a new AWS Glue Crawler.\n",
    "- Fields to fill in:\n",
    "  - Page: Add information about your crawler\n",
    "     - Crawler name: **redshift_sales_analytics_crawler**\n",
    "  - Page: Add a data store\n",
    "     - Choose a data store: **JDBC**\n",
    "     - Connection: **redshiftdb**\n",
    "     - Include path: **sales_analytics_dw/public/%**\n",
    "  - Page: Choose an IAM role\n",
    "     - IAM Role: Choose the IAM Role **###iam_role###**\n",
    "  - Page: Configure the crawler's output\n",
    "     - Database: Click on ‘Add database’ and enter database name as **redshift_sales_analytics**.\n",
    "    \n",
    "- Click on the button ‘Finish’ to create the crawler.\n",
    "- Select the new Crawler and click on 'Run crawler' to run the Crawler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T21:11:00.718467Z",
     "start_time": "2020-05-20T21:11:00.605636Z"
    }
   },
   "outputs": [],
   "source": [
    "## We will simulate the AWS Glue job arguments \n",
    "import sys\n",
    "sys.argv = [\"load_SALES_ORDER_FACT.py\",\"--JOB_NAME\", \"load_SALES_ORDER_FACT\",\"--TempDir\",\"s3://###s3_bucket###/data/temp/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T21:11:04.630440Z",
     "start_time": "2020-05-20T21:11:03.291643Z"
    }
   },
   "outputs": [],
   "source": [
    "## Glue boilerplate code\n",
    "\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "import boto3, json\n",
    "from awsglue.context import GlueContext, DynamicFrame\n",
    "\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "print (args['JOB_NAME']+\" START...\")\n",
    "if 'sc' not in vars(): sc = SparkContext()\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "## Glue boilerplate code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the first table - SALES_ORDER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T21:11:22.547809Z",
     "start_time": "2020-05-20T21:11:09.148993Z"
    }
   },
   "outputs": [],
   "source": [
    "datasource0 = glueContext.create_dynamic_frame_from_options(\"s3\", {'paths': [\"s3://###s3_bucket###/dms-full-load-path/salesdb/SALES_ORDER/\"]}, format=\"parquet\")\n",
    "print \"Rows read from table: SALES_ORDER : \"+str(datasource0.count())\n",
    "datasource0.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the second table - SALES_ORDER_DETAIL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T21:11:24.855592Z",
     "start_time": "2020-05-20T21:11:22.549507Z"
    }
   },
   "outputs": [],
   "source": [
    "datasource1 = glueContext.create_dynamic_frame_from_options(\"s3\", {'paths': [\"s3://###s3_bucket###/dms-full-load-path/salesdb/SALES_ORDER_DETAIL/\"]}, format=\"parquet\")\n",
    "print \"Rows read from table: SALES_ORDER_DETAIL : \"+str(datasource1.count())\n",
    "datasource1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use AWS Glue's Join syntax to join the tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T21:11:40.300649Z",
     "start_time": "2020-05-20T21:11:24.857281Z"
    }
   },
   "outputs": [],
   "source": [
    "datasource2=datasource0.join( [\"ORDER_ID\"],[\"ORDER_ID\"], datasource1, transformation_ctx = \"join\")\n",
    "print \" Rows after Join transform: \"+str(datasource2.count())\n",
    "datasource2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Spark SQL to add some computed columns to the data - EXTENDED_PRICE and PROFIT\n",
    "\n",
    "- EXTENDED_PRICE = QUANTITY * UNIT_PRICE\n",
    "- PROFIT = QUANTITY * ( UNIT_PRICE - SUPPLY_COST )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T21:11:49.780815Z",
     "start_time": "2020-05-20T21:11:44.421575Z"
    }
   },
   "outputs": [],
   "source": [
    "datasource2.toDF().createOrReplaceTempView(\"tbl0\") \n",
    "df1 = spark.sql(\"Select a.*, bround(a.QUANTITY*a.UNIT_PRICE,2) as EXTENDED_PRICE, \\\n",
    "bround(QUANTITY*(UNIT_PRICE-SUPPLY_COST) ,2) as PROFIT, \\\n",
    "DATE_FORMAT(ORDER_DATE,'yyyyMMdd') as DATE_KEY \\\n",
    "from (Select * from tbl0) a\")\n",
    "df1.show(5)\n",
    "datasource4=DynamicFrame.fromDF(df1, glueContext,'datasource4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's map the columns to the target table schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T21:11:52.166191Z",
     "start_time": "2020-05-20T21:11:49.782418Z"
    }
   },
   "outputs": [],
   "source": [
    "applymapping_dynf = ApplyMapping.apply(frame = datasource4, mappings = [(\"DISCOUNT\", \"decimal(10,2)\", \"discount\", \"decimal(10,2)\"), (\"UNIT_PRICE\", \"decimal(10,2)\", \"unit_price\", \"decimal(10,2)\"), (\"TAX\", \"decimal(10,2)\", \"tax\", \"decimal(10,2)\"), (\"SUPPLY_COST\", \"decimal(10,2)\", \"supply_cost\", \"decimal(10,2)\"), (\"PRODUCT_ID\", \"int\", \"product_id\", \"int\"), (\"QUANTITY\", \"int\", \"quantity\", \"int\"), (\"LINE_ID\", \"int\", \"line_id\", \"int\"), (\"LINE_NUMBER\", \"int\", \"line_number\", \"int\"), (\"ORDER_DATE\", \"date\", \"order_date\", \"date\"), (\"SHIP_MODE\", \"string\", \"ship_mode\", \"string\"), (\"SITE_ID\", \"double\", \"site_id\", \"int\"), (\"PROFIT\", \"decimal(10,2)\", \"profit\", \"decimal(10,2)\"),(\"EXTENDED_PRICE\", \"decimal(10,2)\", \"extended_price\", \"decimal(10,2)\"),(\"DATE_KEY\", \"string\", \"date_key\", \"string\"),(\"ORDER_ID\", \"int\", \"order_id\", \"int\")], transformation_ctx = \"applymapping1\")\n",
    "applymapping_dynf.toDF().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's insert the records in the Amazon Redshift target table - sales_order_fact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T21:12:15.698268Z",
     "start_time": "2020-05-20T21:11:52.168015Z"
    }
   },
   "outputs": [],
   "source": [
    "redshift_database_name='redshift_sales_analytics'\n",
    "redshift_table_name='sales_analytics_dw_public_sales_order_fact'\n",
    "\n",
    "datasink3 = glueContext.write_dynamic_frame.from_catalog(frame = applymapping_dynf, database = redshift_database_name, table_name = redshift_table_name, redshift_tmp_dir = args[\"TempDir\"], transformation_ctx = \"datasink3\")\n",
    "datasink3.toDF().show(5)\n",
    "print (\"Rows inserted into Amazon Redshift table : %s : %s \"%(redshift_table_name,str(datasink3.count())))\n",
    "\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the data inserted into the Amazon Redshift table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T21:12:20.148757Z",
     "start_time": "2020-05-20T21:12:19.874545Z"
    }
   },
   "outputs": [],
   "source": [
    "%local \n",
    "import redshift_utils\n",
    "\n",
    "redshift_utils.execute_redshift_query(\"Select count(distinct (order_id)) from sales_order_fact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T23:40:28.710990Z",
     "start_time": "2020-05-06T23:40:28.543206Z"
    }
   },
   "source": [
    "## Execute the Incremental Load Pipeline\n",
    "[(Back to the top)](#top)\n",
    "\n",
    "### Push Incremental data\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Note:</b> Let's run the \"generate_orders(100)\" cell from the 1st notebook in this module to push some Inserts through.</div>\n",
    "\n",
    "**Step 3** : Run the \"generate_orders(100)\" cell in Notebook 1.\n",
    "\n",
    "AWS DMS should replicate the new inserts to our Amazon S3 bucket in a minute. \n",
    "\n",
    "We can run an AWS CLI command to verify that the incremental files has been dropped by AWS DMS to our Amazon S3 bucket:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T16:14:05.353139Z",
     "start_time": "2020-05-22T16:14:04.782079Z"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "aws s3 ls s3://###s3_bucket###/dms-full-load-path/salesdb/SALES_ORDER/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl the DMS Incremental Output Data\n",
    "\n",
    "Let's define and run crawlers to define AWS Glue tables over the incremental data. Note that:\n",
    "\n",
    "- We have an Exclude Pattern (LOAD*) to exclude the AWS DMS Full load file. \n",
    "- We add a Prefix (INCR_) added to the incremental tables so that we can clearly identify them.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"><b>Note:</b> Make sure you do not miss the Exclude Pattern (LOAD*) and the Prefix (INCR_) added to tables in the Crawler definition.</div>\n",
    "\n",
    "**Step 4**: The first crawler should create a table for the changes to the SALES_ORDER table:\n",
    "\n",
    "- Navigate to the AWS Glue console at Services -> AWS Glue\n",
    "- From the left-hand panel menu, navigate to Data Catalog -> Crawlers.\n",
    "- Click on the button ‘Add Crawler’ to create a new AWS Glue Crawler.\n",
    "- Fields to fill in:\n",
    "    - Page: Add information about your crawler\n",
    "        - Crawler name: **incr_sales_order_crawler**\n",
    "    - Page: Add a data store\n",
    "        - Choose a data store: S3\n",
    "        - Include path: **s3://###s3_bucket###/dms-full-load-path/salesdb/SALES_ORDER/**\n",
    "        - Exclude Pattern : **LOAD\\***\n",
    "    - Page: Choose an IAM role\n",
    "       - IAM Role: Choose the **###iam_role###**\n",
    "    - Page: Configure the crawler's output\n",
    "        - Database:  Click on ‘Add database’ and enter database name as **mysql_dms_salesdb**\n",
    "        - Prefix added to tables (optional): **INCR_**\n",
    "- Click on the button ‘Finish’ to create the crawler.\n",
    "- Select the new Crawler and click on Run crawler to run the Crawler.\n",
    "\n",
    "**Step 5**: And the second crawler should create a table for the changes to the SALES_ORDER_DETAIL table:\n",
    "\n",
    "- Navigate to the AWS Glue console at Services -> AWS Glue\n",
    "- From the left-hand panel menu, navigate to Data Catalog -> Crawlers.\n",
    "- Click on the button ‘Add Crawler’ to create a new AWS Glue Crawler.\n",
    "- Fields to fill in:\n",
    "    - Page: Add information about your crawler\n",
    "        - Crawler name: **incr_sales_order_detail_crawler**\n",
    "    - Page: Add a data store\n",
    "        - Choose a data store: S3\n",
    "        - Include path: **s3://###s3_bucket###/dms-full-load-path/salesdb/SALES_ORDER_DETAIL/**\n",
    "        - Exclude Pattern : **LOAD\\***\n",
    "    - Page: Choose an IAM role\n",
    "       - IAM Role: Choose the **###iam_role###**\n",
    "    - Page: Configure the crawler's output\n",
    "        - Database:  Select the database **mysql_dms_salesdb**\n",
    "        - Prefix added to tables (optional): **INCR_**\n",
    "- Click on the button ‘Finish’ to create the crawler.\n",
    "- Select the new Crawler and click on Run crawler to run the Crawler.\n",
    "\n",
    "### Deploy and execute the Incremental Job with AWS Glue Bookmarking enabled\n",
    "\n",
    "Let's now deploy the AWS Glue Job for the incremental load using the AWS SDK below. The following are noteworthy in our AWS Glue Job definition:\n",
    "\n",
    "- The following line enables the AWS Glue Bookmarking feature for the AWS Glue job:\n",
    "\n",
    "```\n",
    "'--job-bookmark-option': 'job-bookmark-enable'\n",
    "```\n",
    "\n",
    " - The following line ensures that AWS Glue creates private ENIs (Elastic Network Interfaces) within the VPC to connect to our Amazon Redshift Database instance:\n",
    "\n",
    "```\n",
    "Connections={'Connections': [redshift_database_connection]},\n",
    "```\n",
    "\n",
    "- The following line allocates the capacity allocated for this AWS Glue Job. We have allocated 3 DPUs. (1 DPU = 4 vCPUs and 16 GB of memory with a 50 GB disk and 2 executors)\n",
    "\n",
    "```\n",
    "MaxCapacity=3.0\n",
    "```\n",
    "\n",
    "\n",
    "You can read more about AWS Glue Job properties and Capacity options here: https://docs.aws.amazon.com/en_us/glue/latest/dg/add-job.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T21:29:13.193777Z",
     "start_time": "2020-05-20T21:29:12.748891Z"
    }
   },
   "outputs": [],
   "source": [
    "%local\n",
    "\n",
    "import boto3\n",
    "\n",
    "acct_number=boto3.client('sts').get_caller_identity().get('Account')\n",
    "bucket='###s3_bucket###'\n",
    "redshift_database_connection='redshiftdb'\n",
    "\n",
    "# Create the AWS Glue Spark Jobs\n",
    "glue = boto3.client(\"glue\")\n",
    "\n",
    "for job_name in ['incr_load_SALES_ORDER_FACT']:\n",
    "    response=glue.create_job(Name=job_name,\n",
    "                         Role=f\"arn:aws:iam::{acct_number}:role/###iam_role###\",\n",
    "                         ExecutionProperty={'MaxConcurrentRuns': 1},\n",
    "                         Command={'Name': 'glueetl',\n",
    "                                  'ScriptLocation': f's3://{bucket}/scripts/{job_name}.py',\n",
    "                                  'PythonVersion': '3'},\n",
    "                         DefaultArguments={'--TempDir': f's3://{bucket}/temp',\n",
    "                                           '--enable-continuous-cloudwatch-log': 'true',\n",
    "                                           '--enable-glue-datacatalog': '',\n",
    "                                           '--enable-metrics': '',\n",
    "                                           '--enable-spark-ui': 'true',\n",
    "                                           '--spark-event-logs-path': f's3://{bucket}/spark_glue_etl_logs/{job_name}',\n",
    "                                           '--job-bookmark-option': 'job-bookmark-enable',\n",
    "                                           '--job-language': 'python',\n",
    "                                           '--S3_BUCKET': bucket },\n",
    "                         Connections={'Connections': [redshift_database_connection]},\n",
    "                         MaxRetries=0,\n",
    "                         Timeout=2880,\n",
    "                         MaxCapacity=3.0,\n",
    "                         GlueVersion='1.0',\n",
    "                         Tags={'Owner': 'Glue_Labs'}\n",
    "                        )\n",
    "    print (response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6**: To run the AWS Glue Job:\n",
    "\n",
    "- Navigate to the AWS Glue console at Services -> AWS Glue\n",
    "- From the left-hand panel menu, navigate to ETL -> Jobs\n",
    "- Select the job 'incr_load_SALES_ORDER_FACT'\n",
    "- And click on the button 'Action -> Run job'\n",
    "- Accept all Default arguments and click on the 'Run job' button.\n",
    "\n",
    "As the job is running, we can inspect the logs and monitor the run from the AWS Glue console.\n",
    "\n",
    "<img src=\"../resources/glue_logs.png\" alt=\"AWS_Glue_Logs\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T16:12:48.159315Z",
     "start_time": "2020-05-22T16:12:45.597073Z"
    }
   },
   "outputs": [],
   "source": [
    "%local \n",
    "redshift_utils.execute_redshift_query(\"Select count(distinct order_id) from sales_order_fact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7** : Finally, let us repeat the steps for some more incremental data:\n",
    "\n",
    "- Insert 100 more orders\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Note:</b> Let's run the \"generate_orders(100)\" cell from the 1st notebook in this module to push some Inserts through.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T16:15:33.142365Z",
     "start_time": "2020-05-22T16:15:32.573026Z"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "aws s3 ls s3://###s3_bucket###/dms-full-load-path/salesdb/SALES_ORDER/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T16:14:43.876339Z",
     "start_time": "2020-05-22T16:14:43.270223Z"
    }
   },
   "source": [
    "- Rerun the AWS Glue Job - incr_load_SALES_ORDER_FACT\n",
    "- Validate the record count in the Amazon Redshift table in the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T16:35:01.497159Z",
     "start_time": "2020-05-22T16:35:01.406293Z"
    }
   },
   "outputs": [],
   "source": [
    "%local \n",
    "redshift_utils.execute_redshift_query(\"Select count(distinct order_id) from sales_order_fact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can see that each run correctly identifies the incremental data and pushes it to the Amazon Redshift table. AWS Glue Bookmarking maintains the state of which file has been processed by each run and ensures that subsequent runs only picks up newer files.\n",
    "\n",
    "You can read more on how AWS Glue Bookmarking works here: https://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "[(Back to the top)](#top)\n",
    "\n",
    "In this notebook, we ran exercises to: \n",
    "\n",
    "1. Load a table to an Amazon Redshift datawarehouse.\n",
    "2. Add computed columns to a dataframe using Spark SQL.\n",
    "3. Read incremental data from Amazon S3 as AWS Glue Tables using AWS Glue Crawlers and Exclusion Patterns.\n",
    "4. Finally, Push the incremental data to a Amazon Redshift table using the AWS Glue Bookmarking feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
