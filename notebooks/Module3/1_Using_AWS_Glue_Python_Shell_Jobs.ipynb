{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using AWS Glue Python Shell Jobs <a name=\"top\"></a>\n",
    "\n",
    "## Table of Contents:\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Activity 1 : Executing Amazon Athena Queries](#Activity-1-:-Executing-Amazon-Athena-Queries)\n",
    "3. [Activity 2 : Deploying the AWS Glue Python Shell Job](#Activity-2-:-Deploying-the-AWS-Glue-Python-Shell-Job)\n",
    "4. [Wrap-up](#Wrap-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "[(Back to the top)](#top)\n",
    "\n",
    "In this notebook, we are going to explore using AWS Glue Python Shell Jobs. Not every use case needs the power of Apache Spark, and Python is a very versatile framework for data processing. Use cases where AWS Glue Python Shell jobs can be used are:\n",
    "    \n",
    "- Orchestrating SQL in databases like Redshift, Aurora etc.\n",
    "- Light-weight ETL using Amazon Athena.\n",
    "- Data processing using Python Pandas or Numpy libraries.\n",
    "- Building Python ML models using Python Scikit-Learn.\n",
    "- And anything else that Python can accomplish.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 1 : Executing Amazon Athena Queries\n",
    "[(Back to the top)](#top)\n",
    "\n",
    "Let's build a SQL driven ETL pipeline that uses the power of Amazon Athena to execute SQL scripts over an Amazon S3 data lake.\n",
    "\n",
    "The architecture diagram for this module looks like below:\n",
    "\n",
    "<img src=\"../resources/module3_architecture_diagram.png\" alt=\"Module3 Architecture Diagram]\" style=\"width: 700px;\"/>\n",
    "\n",
    "Note that AWS Glue Python Shell jobs \n",
    "\n",
    "- Parses the Amazon Athena SQL script and\n",
    "- Submits each SQL query to Amazon Athena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T15:41:01.776356Z",
     "start_time": "2020-05-22T15:41:00.607904Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3,time\n",
    "import pandas as pd\n",
    "\n",
    "defaultdb=\"default\"\n",
    "\n",
    "default_output='s3://###s3_bucket###/athena-sql/data/output/'\n",
    "default_write_location='s3://###s3_bucket###/athena-sql/data/'\n",
    "default_script_location= 's3://###s3_bucket###/scripts/'\n",
    "default_script_logs_location = 's3://###s3_bucket###/athena-sql/logs/'\n",
    "sql_script_file='athena-sql-script.sql'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will write a simple helper function that allows us to send SQL statement to Amazon Athena:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T15:41:04.744621Z",
     "start_time": "2020-05-22T15:41:04.736941Z"
    }
   },
   "outputs": [],
   "source": [
    "def executeQuery(query, database=defaultdb, s3_output=default_output, poll=10):\n",
    "    log_output (\"Executing Query : \\n\") \n",
    "    start = time.time()\n",
    "    log_output (query+\"\\n\")\n",
    "    athena = boto3.client('athena')\n",
    "    response = athena.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={\n",
    "            'Database': database\n",
    "            },\n",
    "        ResultConfiguration={\n",
    "            'OutputLocation': s3_output,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    log_output('Execution ID: ' + response['QueryExecutionId'])\n",
    "    queryExecutionId=response['QueryExecutionId']\n",
    "    state='QUEUED'\n",
    "    while( state=='RUNNING' or state=='QUEUED'):\n",
    "        response = athena.get_query_execution(QueryExecutionId=queryExecutionId)\n",
    "        state=response['QueryExecution']['Status']['State']\n",
    "        log_output (state)\n",
    "        if  state=='RUNNING' or state=='QUEUED':\n",
    "            time.sleep(poll)\n",
    "        elif (state=='FAILED'):\n",
    "             log_output (response['QueryExecution']['Status']['StateChangeReason'])\n",
    "              \n",
    "    done = time.time()\n",
    "    log_output (\"Elapsed Time (in seconds) : %f \\n\"%(done - start))\n",
    "    return response\n",
    "\n",
    "def log_output(s):\n",
    "    print (s)\n",
    "    log_output_string.append(s)\n",
    "    \n",
    "def read_from_athena(sql):\n",
    "    response=executeQuery(sql)\n",
    "    return pd.read_csv(response['QueryExecution']['ResultConfiguration']['OutputLocation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script we are going to use is here : [athena-sql-script.sql](athena-sql-script.sql) \n",
    "\n",
    "We will read the SQL file from our S3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T15:41:08.461964Z",
     "start_time": "2020-05-22T15:41:08.254554Z"
    }
   },
   "outputs": [],
   "source": [
    "s3_location= default_script_location+sql_script_file\n",
    "bucket_name,script_location=s3_location.split('/',2)[2].split('/',1)\n",
    "print (bucket_name)\n",
    "print (script_location)\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "fileobj = s3.get_object(Bucket=bucket_name,Key=script_location)\n",
    "contents = fileobj['Body'].read().decode('utf-8')\n",
    "contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's execute the script. This step issues the SQL commands to Amazon Athena and should take around 2 mins. You can navigate to the Amazon Athena console and view the queries being submitted:\n",
    "\n",
    "- Navigate to the AWS Athena Console\n",
    "- Click on the History tabe to view the queries submitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T15:47:41.818365Z",
     "start_time": "2020-05-22T15:46:20.974895Z"
    }
   },
   "outputs": [],
   "source": [
    "log_output_string=[]\n",
    "for sql in str(contents).split(\";\")[:-1]:\n",
    "    if len(sql) > 0: \n",
    "        response=executeQuery(sql)\n",
    "print (\"\\n\".join(log_output_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pipeline is complete and we can see the results of our SQL Script run above - the Amazon Athena Execution Ids of each query as well the execution time for each query.\n",
    "\n",
    "Python shell jobs in AWS Glue come pre-loaded with libraries such as the Boto3, NumPy, SciPy, Pandas and others. You can load any custom Python libraries into the AWS Glue Python Shell environment packaged as an .egg or a .whl file.\n",
    "\n",
    "You can read more about AWS Glue Python Shell features here: https://docs.aws.amazon.com/glue/latest/dg/add-job-python.html\n",
    "\n",
    "Let us read the final report data as a Pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T15:48:01.224563Z",
     "start_time": "2020-05-22T15:47:50.566052Z"
    }
   },
   "outputs": [],
   "source": [
    "read_from_athena(\"Select * from default.top_flight_delays_report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 2 : Deploying the AWS Glue Python Shell Job\n",
    "[(Back to the top)](#top)\n",
    "\n",
    "As a final step, we will deploy this pipeline as an AWS Glue Python Shell job and execute it.\n",
    "\n",
    "Note that an AWS Glue Python Shell job can use 1 DPU (Data Processing Unit) or 0.0625 DPU (which is 1/16 DPU). A single DPU provides processing capacity that consists of 4 vCPUs of compute and 16 GB of memory. For our use case, 0.0625 DPU is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T16:02:43.177406Z",
     "start_time": "2020-05-22T16:02:42.684977Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "acct_number=boto3.client('sts').get_caller_identity().get('Account')\n",
    "bucket='###s3_bucket###'\n",
    "\n",
    "# Create the Glue Spark Jobs\n",
    "glue = boto3.client(\"glue\")\n",
    "\n",
    "for job_name in ['Build_Top_Flight_Delays_Report']:\n",
    "    response=glue.create_job(Name=job_name,\n",
    "                         Role=f\"arn:aws:iam::{acct_number}:role/###iam_role###\",\n",
    "                         ExecutionProperty={'MaxConcurrentRuns': 1},\n",
    "                         Command={'Name': 'pythonshell',\n",
    "                                  'ScriptLocation': f's3://{bucket}/scripts/{job_name}.py',\n",
    "                                  'PythonVersion': '3'},\n",
    "                         DefaultArguments={'--TempDir': f's3://{bucket}/temp',\n",
    "                                           '--enable-metrics': '',\n",
    "                                           '--job-language': 'python',\n",
    "                                           '--S3_BUCKET': bucket },\n",
    "                         MaxRetries=0,\n",
    "                         Timeout=2880,\n",
    "                         MaxCapacity=0.0625,\n",
    "                         GlueVersion='1.0',\n",
    "                         Tags={'Owner': 'AWS_Glue_Labs'}\n",
    "                        )\n",
    "    print (response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the AWS Glue job is deployed, let's execute it:\n",
    "\n",
    "- Navigate to the AWS Glue Console -> Jobs. \n",
    "- Select the 'Build_Top_Flight_Delays_Report' Glue Jobs and \n",
    "- Click on the 'Action -> Run Job' option to execute the job.  \n",
    "\n",
    "We can monitor the Execution Details from the AWS Glue console and once the job is over view the logs by clicking on the 'Logs' link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "[(Back to the top)](#top)\n",
    "\n",
    "In this notebook, we ran exercises to : \n",
    "\n",
    "1. Execute a light-weight SQL driven ETL pipeline using Amazon Athena and\n",
    "2. Deployed the pipeline as a AWS Glue Python Shell Job.\n",
    "\n",
    "We hope this lab helped you to understand how to leverage the simplicity and power of Python in your Data Pipelines using AWS Glue Python Shell Jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
